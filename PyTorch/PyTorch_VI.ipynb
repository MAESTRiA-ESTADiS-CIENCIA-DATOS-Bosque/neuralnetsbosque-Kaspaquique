{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**PyTorch VI**\n",
        "Autor: Jheremy Reyes,\n",
        "\n",
        "estudiante de matemáticas,\n",
        "\n",
        "Universidad El Bosque"
      ],
      "metadata": {
        "id": "_Tq6RErdIu1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Implementación de un Perceptrón Multicapa (MLP) en PyTorch**"
      ],
      "metadata": {
        "id": "P4hmgQeMI7xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un Perceptrón Multicapa (MLP) es una red neuronal totalmente conectada (Feedforward Neural Network). Consiste en:\n",
        "\n",
        "Una capa de entrada.\n",
        "\n",
        "Una o más capas ocultas con activación no lineal (ReLU, Sigmoid, etc.).\n",
        "\n",
        "Una capa de salida para clasificación o regresión.\n",
        "\n",
        "A continuación, te muestro cómo implementar un MLP en PyTorch para clasificación binaria."
      ],
      "metadata": {
        "id": "4trNMW9oI8YA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0QlhhsOIR_L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Definir las capas de la red neuronal\n",
        "layer1 = nn.Linear(3, 2)  # Entrada de tamaño 3, salida de tamaño 2\n",
        "layer2 = nn.Linear(2, 3)  # Entrada de tamaño 2, salida de tamaño 3\n",
        "layer3 = nn.Linear(3, 4)  # Entrada de tamaño 3, salida de tamaño 4\n",
        "\n",
        "# Funciones de activación\n",
        "relu = nn.ReLU()\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "# Crear un tensor de prueba (equivalente a tf.ones((3,3)))\n",
        "x = torch.ones((3, 3))\n",
        "\n",
        "# Aplicar las capas secuencialmente\n",
        "y = layer3(sigmoid(layer2(relu(layer1(x)))))\n",
        "print(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo equivalente en PyTorch\n",
        "class MyFirstModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyFirstModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(3, 2)  # Equivalente a Dense(2, activation=\"relu\")\n",
        "        self.layer2 = nn.Linear(2, 3)  # Equivalente a Dense(3, activation=\"sigmoid\")\n",
        "        self.layer3 = nn.Linear(3, 4)  # Equivalente a Dense(4)\n",
        "\n",
        "        # Funciones de activación\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))   # Aplicar ReLU después de la primera capa\n",
        "        x = self.sigmoid(self.layer2(x))  # Aplicar Sigmoid después de la segunda capa\n",
        "        x = self.layer3(x)  # Tercera capa sin activación\n",
        "        return x\n",
        "\n",
        "# Crear una instancia del modelo\n",
        "model = MyFirstModel()\n",
        "\n",
        "# Crear un tensor de entrada (equivalente a tf.ones((3,3)))\n",
        "x = torch.ones((3, 3))\n",
        "\n",
        "# Pasar los datos a través del modelo\n",
        "y = model(x)\n",
        "print(y)\n"
      ],
      "metadata": {
        "id": "rXJ6P0DaJEhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de capas en el modelo PyTorch\n",
        "print(list(model.children()))  # Obtiene las capas definidas en el modelo\n"
      ],
      "metadata": {
        "id": "TjSBQCCtJIGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Acceder a los pesos de la tercera capa (layer3 en este caso)\n",
        "weights = model.layer3.weight  # Pesos de la capa\n",
        "bias = model.layer3.bias  # Bias de la capa\n",
        "\n",
        "# Imprimir pesos y bias\n",
        "print(\"Pesos de la capa 3:\\n\", weights)\n",
        "print(\"Bias de la capa 3:\\n\", bias)\n"
      ],
      "metadata": {
        "id": "KbUDlMf4JKyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Mostrar el resumen del modelo\n",
        "summary(model, input_size=(3, 3))  # Tamaño de entrada similar a Keras\n"
      ],
      "metadata": {
        "id": "jiFAmCUSJOa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la red en InitialModel, equivalente al modelo Sequential en Keras.\n",
        "\n",
        "nn.Linear(in_features, out_features) se usa en lugar de Dense(units, activation=...).\n",
        "\n",
        "forward() maneja la propagación, aplicando las activaciones manualmente.\n",
        "\n",
        "El resumen se genera con torchinfo.summary(), similar a model.summary()."
      ],
      "metadata": {
        "id": "LJattN9vJRkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo en PyTorch\n",
        "class InitialModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(InitialModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(3, 2)  # Equivalente a Dense(2, activation=\"relu\")\n",
        "        self.layer2 = nn.Linear(2, 3)  # Equivalente a Dense(3, activation=\"sigmoid\")\n",
        "        self.layer3 = nn.Linear(3, 4)  # Equivalente a Dense(4)\n",
        "\n",
        "        # Funciones de activación\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))   # Aplicar ReLU después de la primera capa\n",
        "        x = self.sigmoid(self.layer2(x))  # Aplicar Sigmoid después de la segunda capa\n",
        "        x = self.layer3(x)  # Tercera capa sin activación\n",
        "        return x\n",
        "\n",
        "# Crear el modelo\n",
        "initial_model = InitialModel()\n",
        "\n",
        "# Resumen del modelo\n",
        "summary(initial_model, input_size=(1, 3))  # Input de tamaño (batch_size, features)"
      ],
      "metadata": {
        "id": "F9o6CCuoJU39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir un tensor de entrada de prueba\n",
        "x = torch.randn(1, 3)  # Simula una entrada de tamaño (1,3)\n",
        "\n",
        "# Pasar la entrada al modelo\n",
        "output = initial_model(x)\n",
        "\n",
        "# Mostrar la forma de la entrada\n",
        "print(\"Forma de la entrada:\", x.shape)\n"
      ],
      "metadata": {
        "id": "FysxunmqJYAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir un tensor de entrada de prueba\n",
        "x = torch.randn(1, 3)  # Simula una entrada de tamaño (batch=1, features=3)\n",
        "\n",
        "# Obtener la salida del modelo\n",
        "output = initial_model(x)\n",
        "\n",
        "# Mostrar la salida y su forma\n",
        "print(\"Salida del modelo:\\n\", output)\n",
        "print(\"Forma de la salida:\", output.shape)\n"
      ],
      "metadata": {
        "id": "LsMimKz9JbIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En TensorFlow/Keras, model.trainable_variables devuelve una lista con los pesos y sesgos entrenables del modelo.\n",
        "\n",
        "En PyTorch, el equivalente es model.parameters(), que devuelve los tensores de los parámetros entrenables."
      ],
      "metadata": {
        "id": "z5LDiqXtJhiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo en PyTorch\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(3, 2)  # Equivalente a Dense(2, activation=\"relu\")\n",
        "        self.layer2 = nn.Linear(2, 3)  # Equivalente a Dense(3, activation=\"sigmoid\")\n",
        "        self.layer3 = nn.Linear(3, 4)  # Equivalente a Dense(4, activation=\"relu\")\n",
        "\n",
        "        # Activaciones\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))   # ReLU después de la primera capa\n",
        "        x = self.sigmoid(self.layer2(x))  # Sigmoid después de la segunda capa\n",
        "        x = self.relu(self.layer3(x))  # ReLU después de la tercera capa\n",
        "        return x\n",
        "\n",
        "# Crear el modelo\n",
        "model = MyModel()\n",
        "\n",
        "# Resumen del modelo\n",
        "summary(model, input_size=(1, 3))  # Input con batch_size=1 y 3 features"
      ],
      "metadata": {
        "id": "0lCXWxPjJdxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener los parámetros entrenables del modelo\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:  # Solo muestra los parámetros entrenables\n",
        "        print(f\"Nombre: {name}, Forma: {param.shape}\")"
      ],
      "metadata": {
        "id": "dWIbJJeYJkxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En TensorFlow/Keras, model.layers permite iterar sobre las capas de un modelo Sequential, y layer.trainable indica si los parámetros de la capa se pueden entrenar.\n",
        "\n",
        "En PyTorch, el equivalente es iterar sobre model.children() y verificar requires_grad en los parámetros de cada capa."
      ],
      "metadata": {
        "id": "wwBPAqU-JoPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterar sobre las capas del modelo y verificar si son entrenables\n",
        "for name, layer in model.named_children():\n",
        "    trainable = any(param.requires_grad for param in layer.parameters())\n",
        "    print(f\"Capa: {name}, Entrenable: {trainable}\")"
      ],
      "metadata": {
        "id": "_jFDVewsJrAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En TensorFlow/Keras, la propiedad model.trainable = False congela todas las capas del modelo, haciendo que model.trainable_variables devuelva una lista vacía.\n",
        "\n",
        "En PyTorch, el equivalente es desactivar requires_grad en todos los parámetros del modelo."
      ],
      "metadata": {
        "id": "RZexYy_WJvex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Congelar todos los parámetros del modelo\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Verificar los parámetros entrenables después de la congelación\n",
        "trainable_params = [param for param in model.parameters() if param.requires_grad]\n",
        "print(\"Parámetros entrenables:\", len(trainable_params))  # Debe ser 0 si todo está congelado"
      ],
      "metadata": {
        "id": "7gAfd0BIJyrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En TensorFlow/Keras, puedes iterar sobre model.layers y verificar si cada capa es entrenable con layer.trainable.\n",
        "\n",
        "En PyTorch, no existe model.layers, pero puedes usar model.children() o model.named_children() para acceder a las capas y verificar si sus parámetros son entrenables."
      ],
      "metadata": {
        "id": "vhjs4aY3J180"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterar sobre las capas del modelo y verificar si son entrenables\n",
        "for name, layer in model.named_children():\n",
        "    trainable = any(param.requires_grad for param in layer.parameters())\n",
        "    print(f\"Capa: {name}, Entrenable: {trainable}\")"
      ],
      "metadata": {
        "id": "JROPxTCDJ4nI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo en PyTorch\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(3, 2)  # Equivalente a Dense(2, activation=\"relu\")\n",
        "        self.layer2 = nn.Linear(2, 3)  # Equivalente a Dense(3, activation=\"sigmoid\")\n",
        "        self.layer3 = nn.Linear(3, 4)  # Equivalente a Dense(4, activation=\"relu\")\n",
        "        self.layer4 = nn.Linear(4, 5)  # Equivalente a Dense(5, activation=\"relu\")\n",
        "        self.layer5 = nn.Linear(5, 6)  # Equivalente a Dense(6, activation=\"sigmoid\")\n",
        "        self.layer6 = nn.Linear(6, 7)  # Equivalente a Dense(7, activation=\"relu\")\n",
        "\n",
        "        # Activaciones\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))   # ReLU después de la primera capa\n",
        "        x = self.sigmoid(self.layer2(x))  # Sigmoid después de la segunda capa\n",
        "        x = self.relu(self.layer3(x))  # ReLU después de la tercera capa\n",
        "        x = self.relu(self.layer4(x))  # ReLU después de la cuarta capa\n",
        "        x = self.sigmoid(self.layer5(x))  # Sigmoid después de la quinta capa\n",
        "        x = self.relu(self.layer6(x))  # ReLU después de la sexta capa\n",
        "        return x\n",
        "\n",
        "# Crear el modelo\n",
        "model = MyModel()\n",
        "\n",
        "# Resumen del modelo\n",
        "summary(model, input_size=(1, 3))  # Input con batch_size=1 y 3 features"
      ],
      "metadata": {
        "id": "b5RI6PiXJ7hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En TensorFlow/Keras, la API funcional permite extraer características de un modelo definiendo una nueva arquitectura con entradas y salidas específicas.\n",
        "\n",
        "En PyTorch, puedes hacer algo similar accediendo directamente a las capas del modelo en forward()."
      ],
      "metadata": {
        "id": "pJmuIKHUJ-Xn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo base en PyTorch\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(3, 2)\n",
        "        self.layer2 = nn.Linear(2, 3)\n",
        "        self.layer3 = nn.Linear(3, 4)\n",
        "        self.layer4 = nn.Linear(4, 5)  # Salida deseada del extractor\n",
        "        self.layer5 = nn.Linear(5, 6)\n",
        "        self.layer6 = nn.Linear(6, 7)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.sigmoid(self.layer2(x))\n",
        "        x = self.relu(self.layer3(x))\n",
        "        features = self.relu(self.layer4(x))  # Extracción en \"layer4\"\n",
        "\n",
        "        if return_features:\n",
        "            return features  # Devuelve solo hasta \"layer4\"\n",
        "\n",
        "        x = self.sigmoid(self.layer5(features))\n",
        "        x = self.relu(self.layer6(x))\n",
        "        return x\n",
        "\n",
        "# Crear el modelo base\n",
        "model = MyModel()\n",
        "\n",
        "# Crear el \"feature extractor\"\n",
        "def feature_extractor(x):\n",
        "    return model(x, return_features=True)\n",
        "\n",
        "# Prueba con una entrada\n",
        "x_test = torch.randn(1, 3)  # Entrada de prueba (batch_size=1, features=3)\n",
        "features = feature_extractor(x_test)\n",
        "\n",
        "print(\"Salida del extractor de características:\\n\", features)"
      ],
      "metadata": {
        "id": "OZfvFCfHKB1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En Keras, feature_extractor.summary() muestra el resumen del modelo intermedio.\n",
        "\n",
        "En PyTorch, puedes usar torchinfo.summary() para obtener información similar."
      ],
      "metadata": {
        "id": "hdSCZcnUKFv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Resumen del extractor de características\n",
        "summary(model, input_size=(1, 3), depth=4)\n"
      ],
      "metadata": {
        "id": "N5Afp-OWKJOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En TensorFlow/Keras, tf.keras.utils.plot_model() genera un diagrama visual del modelo.\n",
        "\n",
        "En PyTorch, no hay un equivalente directo pero se puede usar torchviz o hiddenlayer para visualizar la arquitectura."
      ],
      "metadata": {
        "id": "nz8FqrNEKMAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchviz"
      ],
      "metadata": {
        "id": "g0jTtpd_KOl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchviz import make_dot\n",
        "\n",
        "# Entrada de ejemplo\n",
        "x_sample = torch.randn(1, 3)\n",
        "\n",
        "# Obtener la salida del modelo\n",
        "y_sample = model(x_sample)\n",
        "\n",
        "# Generar el diagrama\n",
        "dot = make_dot(y_sample, params=dict(model.named_parameters()))\n",
        "\n",
        "# Guardar la imagen\n",
        "dot.format = \"png\"\n",
        "dot.render(\"model\")\n",
        "\n",
        "# Mostrar en Jupyter Notebook (si lo estás usando)\n",
        "dot\n"
      ],
      "metadata": {
        "id": "7GvnhZaeKSB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Linear(in_features, out_features) es el equivalente de Dense(units).\n",
        "\n",
        "Activaciones (ReLU y Softmax) se aplican en forward(), ya que PyTorch no las incluye dentro de Linear().\n",
        "\n",
        "Resumen del modelo se obtiene con torchinfo.summary()."
      ],
      "metadata": {
        "id": "9GXd-3JQKVAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo en PyTorch\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)  # Capa densa de 64 neuronas\n",
        "        self.fc2 = nn.Linear(64, 64)   # Otra capa de 64 neuronas\n",
        "        self.fc3 = nn.Linear(64, 10)   # Capa de salida con 10 clases\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # Softmax en la dimensión de las clases\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.softmax(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# Crear el modelo\n",
        "model = MyModel()\n",
        "\n",
        "# Mostrar resumen del modelo\n",
        "summary(model, input_size=(1, 784))\n"
      ],
      "metadata": {
        "id": "K6P5pDcKKYFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se aplanan las imágenes de (32, 32, 3) a 3072 (x.view(x.size(0), -1)).\n",
        "\n",
        "Se replican las conexiones residuales (torch.add()) en block_2_output y block_3_output.\n",
        "\n",
        "Las activaciones (ReLU, Softmax) se aplican manualmente en forward().\n",
        "\n",
        "Se mantiene Dropout(0.5) antes de la salida.\n",
        "\n",
        "Resumen del modelo se obtiene con torchinfo.summary()."
      ],
      "metadata": {
        "id": "1LfEMEzGKfgb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo en PyTorch\n",
        "class VeryDenseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VeryDenseNet, self).__init__()\n",
        "\n",
        "        # Bloque 1\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 64)  # block_1_output\n",
        "\n",
        "        # Bloque 2\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 64)\n",
        "\n",
        "        # Bloque 3\n",
        "        self.fc6 = nn.Linear(64, 32)\n",
        "        self.fc7 = nn.Linear(32, 64)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc8 = nn.Linear(64, 16)\n",
        "        self.fc9 = nn.Linear(16, 32)\n",
        "        self.fc10 = nn.Linear(32, 256)\n",
        "        self.fc_out = nn.Linear(256, 10)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Aplanar la imagen (32x32x3 → 3072)\n",
        "\n",
        "        # Bloque 1\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        block_1_output = F.relu(self.fc3(x))\n",
        "\n",
        "        # Bloque 2 con skip connection\n",
        "        x = F.relu(self.fc4(block_1_output))\n",
        "        x = F.relu(self.fc5(x))\n",
        "        block_2_output = torch.add(x, block_1_output)  # Residual sum\n",
        "\n",
        "        # Bloque 3 con skip connection\n",
        "        x = F.relu(self.fc6(block_2_output))\n",
        "        x = F.relu(self.fc7(x))\n",
        "        block_3_output = torch.add(x, block_2_output)  # Residual sum\n",
        "\n",
        "        # Capas finales\n",
        "        x = F.relu(self.fc8(block_3_output))\n",
        "        x = F.relu(self.fc9(x))\n",
        "        x = F.relu(self.fc10(x))\n",
        "        x = self.dropout(x)  # Dropout antes de la salida\n",
        "        x = F.softmax(self.fc_out(x), dim=1)  # Softmax para clasificación\n",
        "\n",
        "        return x\n",
        "\n",
        "# Crear el modelo\n",
        "model = VeryDenseNet()\n",
        "\n",
        "# Resumen del modelo\n",
        "summary(model, input_size=(1, 3, 32, 32))\n"
      ],
      "metadata": {
        "id": "KV3TcYXcKkLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torch.randn(1, 3, 32, 32) crea un tensor de entrada con el mismo tamaño que en Keras.\n",
        "\n",
        "make_dot(y_sample, params=dict(model.named_parameters())) genera la estructura del modelo.\n",
        "\n",
        "dot.render(\"model_visualization\") guarda el modelo en un archivo .png.\n",
        "\n",
        "dot lo muestra en un Jupyter Notebook."
      ],
      "metadata": {
        "id": "KJk1bUDOKnx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una entrada de prueba (batch de 1 imagen de 32x32x3)\n",
        "x_sample = torch.randn(1, 3, 32, 32)\n",
        "\n",
        "# Obtener la salida del modelo\n",
        "y_sample = model(x_sample)\n",
        "\n",
        "# Generar el diagrama de flujo del modelo\n",
        "dot = make_dot(y_sample, params=dict(model.named_parameters()))\n",
        "\n",
        "# Guardar la imagen\n",
        "dot.format = \"png\"\n",
        "dot.render(\"model_visualization\")\n",
        "\n",
        "# Mostrar en Jupyter Notebook\n",
        "dot"
      ],
      "metadata": {
        "id": "eSRHMzN0Kqk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Personalizar Layers y Models**"
      ],
      "metadata": {
        "id": "9Ntd9LJ-KvOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se usa nn.Module para definir la capa personalizada.\n",
        "\n",
        "Se inicializan los pesos (self.w) y el sesgo (self.b) en la primera pasada dentro de forward(), equivalente a build() en Keras.\n",
        "\n",
        "Se aplica la operación torch.matmul(inputs, self.w) + self.b, equivalente a tf.matmul(inputs, self.w) + self.b.\n",
        "\n",
        "Se crean parámetros entrenables (nn.Parameter) para que PyTorch los optimice automáticamente"
      ],
      "metadata": {
        "id": "S76cig3jKv4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la clase de la capa personalizada\n",
        "class MyLayer(nn.Module):\n",
        "    def __init__(self, units=1):\n",
        "        super(MyLayer, self).__init__()\n",
        "        self.units = units\n",
        "        self.w = None  # Se inicializa en forward()\n",
        "        self.b = None\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.w is None:  # Inicializar pesos en la primera pasada (similar a 'build' en Keras)\n",
        "            input_dim = inputs.shape[-1]\n",
        "            self.w = nn.Parameter(torch.randn(input_dim, self.units))  # Pesos inicializados normal\n",
        "            self.b = nn.Parameter(torch.zeros(self.units))  # Sesgo inicializado en ceros\n",
        "        return torch.matmul(inputs, self.w) + self.b\n",
        "\n",
        "# Crear una instancia de la capa con 8 unidades\n",
        "linear_layer = MyLayer(8)\n",
        "\n",
        "# Crear una entrada de prueba (tensor de 1x2)\n",
        "x = torch.ones((1, 2))\n",
        "\n",
        "# Pasar la entrada por la capa\n",
        "y = linear_layer(x)\n",
        "y"
      ],
      "metadata": {
        "id": "8U1wluSVK1T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se usa nn.Linear() en lugar de tf.keras.layers.Dense().\n",
        "\n",
        "Se aplican activaciones manualmente (F.relu() y F.elu()).\n",
        "\n",
        "Se maneja training=False congelando los parámetros de dense3 con param.requires_grad = False.\n",
        "\n",
        "Se usa torchinfo.summary() para imprimir un resumen del modelo (similar a model.summary() en Keras)."
      ],
      "metadata": {
        "id": "6LNopwK1K4qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la clase del modelo personalizado\n",
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.dense1 = nn.Linear(2, 2)  # Entrada de tamaño 2, salida de tamaño 2\n",
        "        self.dense2 = nn.Linear(2, 4)\n",
        "        self.dense3 = nn.Linear(4, 8)\n",
        "\n",
        "    def forward(self, inputs, training=False):\n",
        "        x = F.relu(self.dense1(inputs))\n",
        "        x = F.relu(self.dense2(x))\n",
        "        x = F.elu(self.dense3(x))\n",
        "\n",
        "        if not training:\n",
        "            for param in self.dense3.parameters():\n",
        "                param.requires_grad = False  # Congelar capa densa3 si no está en entrenamiento\n",
        "\n",
        "        return x\n",
        "\n",
        "# Crear el modelo\n",
        "model = MyModel()\n",
        "\n",
        "# Crear una entrada de prueba\n",
        "x = torch.ones((1, 2)) * 2\n",
        "\n",
        "# Pasar la entrada por el modelo\n",
        "y = model(x, training=False)\n",
        "\n",
        "print('Resultado:', y)\n",
        "\n",
        "# Mostrar resumen del modelo\n",
        "from torchinfo import summary\n",
        "summary(model, input_size=(1, 2))"
      ],
      "metadata": {
        "id": "BTa5xO52K7Hi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}